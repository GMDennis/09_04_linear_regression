{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn import metrics\n",
    "from sklearn import svm, datasets, linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV, SGDClassifier, LogisticRegressionCV, RidgeClassifierCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, LeaveOneOut, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import load_boston, load_digits, load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 12, 10\n",
    "\n",
    "# Tip: if you want the generated figure to be large, re-run this cell before the beginning of every exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## a. Cross validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### cross_validate(estimator, X, y=None, scoring=None, cv=’warn’, return_train_score=False, return_estimator=False)\n",
    "*  Method to evaluate metric(s) by cross-validation and also record fit/score times. \n",
    " \n",
    "* #### Parameters\n",
    " 1.    estimator: the object to use to fit the data\n",
    " 2.    X: the data to fit, can be a list or an array.\n",
    " 3.    y: the target variable to try to predict in the case of supervise learning\n",
    " 4.    scoring: string, callable, list/tuple, dict or None\n",
    "       * string: define model evaluation metric\n",
    "           * Classification:\n",
    "               * 'accuracy': accuracy classification score\n",
    "               * 'f1': F1-score\n",
    "               * 'precision': precision ratio true_positive/(true_positive + false_positive)\n",
    "               * 'recall': recall ratio true_positive/(true_positive + false_negative)\n",
    "           * Regression:\n",
    "               * 'explained_variance': explained variance regression score function.\n",
    "               * 'neg_mean_absolute_error': mean absolute error regression loss.\n",
    "               * 'neg_mean_squared_error': mean squared error regression loss.\n",
    "               \n",
    "       * callable: a function to evaluate the predictions on the test set\n",
    "       * list/tuple: for evaluating multiple metrics, list/tuple of (unique) strings\n",
    "       * dict: for evaluating multiple metrics, dictionary with names as keys and callables as values.\n",
    " 5.    cv: int, determine the cross-validation splitting strategy. \n",
    "       * None: default 3-fold cross validation.\n",
    "       * integer: specify the number of folds in a KFold\n",
    "       * an iterable yielding (train,test) splits as arrays of indices.\n",
    " 6.    return_train_score: whether to include train scores.\n",
    " 7.    return_estimator: whether to return the estimators fitted on each split.\n",
    " \n",
    "*  #### Returns (of the method): scores ~ a dict of float arrays\n",
    " *    test_score: the score array for test scores on each cross-alidation split.\n",
    " *    train_score: the score array for train scores on each crosd-validation split.\n",
    " *    fit_time: the time for fitting the estimator on the train set for each cross-validation split.\n",
    " *    score_time: the time for scoring the estimator on the test set for each cross-validation split.\n",
    " *    estimator: the estimator objects for each cross-validation split. only when return_estimator=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fit_time', 'score_time', 'test_score'])\n"
     ]
    }
   ],
   "source": [
    "# Example: Cross-validation on regression problem\n",
    "\n",
    "# Data\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "\n",
    "# Lasso Linear Regression object\n",
    "lasso = Lasso()\n",
    "\n",
    "# Use 3-fold cross-validation on the Lasso linear regression problem \n",
    "cv_results = cross_validate(lasso, X, y, cv=3)\n",
    "\n",
    "# Let us see what values are available\n",
    "print(cv_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33150734 0.08022311 0.03531764]\n"
     ]
    }
   ],
   "source": [
    "# Example: Cross-validation on regression problem (cont)\n",
    "\n",
    "# Check the test score on 3 folds:\n",
    "print(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29177858 0.35449689 0.38995421 0.20300574]\n",
      "[-4047.7288045  -3850.01747559 -3592.51190783 -3855.59792991]\n",
      "[ 0.3392459   0.12286347  0.16482017 -0.04610521]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Cross-validation on regresison problem (cont)\n",
    "\n",
    "# Let us try to apply Lasso Linear Regression on the same data but with 4-fold cross validation\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "# Hint: we use evaluation metrics 'r2' and 'neg_mean_squarred_score'\n",
    "# Hint: we also want to retrieve the training score\n",
    "scores = None\n",
    "# Print the train score applicable to r2\n",
    "print(None)    \n",
    "# Hint: print the train score applicable to neg_mean_squared_error\n",
    "print(None)     \n",
    "# Hint: print the test score applicable to r2\n",
    "print(None)    \n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## b. Splitting Train and Test sets\n",
    "\n",
    "------------------\n",
    "\n",
    "### train_test_split(arrays, test_size=None, train_size=None, shuffle=True)\n",
    "*  Split array or matrices into random train and test subsets.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    arrays: sequence of indexables with same length/shape[0]\n",
    "       can be lists, numpy arrays, scip-sparse matices or pandas dataframes.\n",
    " 2.    test_size: float, int or None\n",
    "         * float: between 0 and 1, proportion of the dataset to be in the test split.\n",
    "         * int: absolute number of test samples\n",
    "         * None: automatically set to be complement of train_size; if train_size is also None, then the ratio is set to be 0.25\n",
    " 3.    train_size: float, int or None\n",
    "         * float: between 0 and 1, proportion of the dataset to be in the train split.\n",
    "         * int: absolute number of train samples\n",
    "         * None: automatically set to be complement of test_size.\n",
    " 4.    shuffle: whether to shuffle the data before splitting.\n",
    " \n",
    "*  #### Returns: splitting\n",
    "        List containing train-test split of inputs.\n",
    "        e.x: X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>fixed acidity</td>\n",
       "      <td>0.005743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>volatile acidity</td>\n",
       "      <td>-1.229988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>citric acid</td>\n",
       "      <td>-0.103239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>residual sugar</td>\n",
       "      <td>0.015001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>chlorides</td>\n",
       "      <td>-1.829490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>free sulfur dioxide</td>\n",
       "      <td>0.002147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>total sulfur dioxide</td>\n",
       "      <td>-0.003042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>density</td>\n",
       "      <td>-0.111951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pH</td>\n",
       "      <td>-0.471583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sulphates</td>\n",
       "      <td>0.818644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>alcohol</td>\n",
       "      <td>0.283457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Coefficient\n",
       "fixed acidity            0.005743\n",
       "volatile acidity        -1.229988\n",
       "citric acid             -0.103239\n",
       "residual sugar           0.015001\n",
       "chlorides               -1.829490\n",
       "free sulfur dioxide      0.002147\n",
       "total sulfur dioxide    -0.003042\n",
       "density                 -0.111951\n",
       "pH                      -0.471583\n",
       "sulphates                0.818644\n",
       "alcohol                  0.283457"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: Linear Regression on predicting wine quality revisited\n",
    "\n",
    "# RELOAD EVERYTHING\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('winequality.csv')\n",
    "\n",
    "# Clean up the data by removing all null values\n",
    "dataset = dataset.fillna(method='ffill')\n",
    "\n",
    "# Devide attributes/features and output vales\n",
    "# Features\n",
    "X = dataset[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', \n",
    "             'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates','alcohol']].values\n",
    "# Output values\n",
    "y = dataset['quality'].values\n",
    "\n",
    "# Split the dataset into 70% train, 30% test\n",
    "### YOUR CODE HERE. 1 line of code. There are more than 1 way to split in such manner.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# L2-regularization Linear Regression\n",
    "### YOUR CODE HERE. Fill in the \"None\". 2 lines of code.\n",
    "# Hint: train L2-regularization Linear Regression with 0.1 as the regularization weight\n",
    "regressor = None\n",
    "None\n",
    "\n",
    "# Print coefficients associated with 12 features. Hint: the first parameter of the function must be the coefficients\n",
    "coeff_df = pd.DataFrame(None, dataset.columns[:-1], columns=['Coefficient']) \n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# Show the learned coefficients\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.788372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.048661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6.574231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5.394111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.910120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5.060175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5.404963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6.002679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4.823235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4.966325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5.339087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5.479348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>5.767514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>5.022486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>5.527966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>6.363936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>6.796256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>5.828408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>5.928893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>5.092832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6.294611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5.207945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>5.637689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>6.106051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>5.531360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted\n",
       "0        6   5.788372\n",
       "1        5   5.048661\n",
       "2        7   6.574231\n",
       "3        6   5.394111\n",
       "4        5   5.910120\n",
       "5        6   5.060175\n",
       "6        5   5.404963\n",
       "7        6   6.002679\n",
       "8        4   4.823235\n",
       "9        5   4.966325\n",
       "10       5   5.339087\n",
       "11       5   5.479348\n",
       "12       6   5.767514\n",
       "13       5   5.022486\n",
       "14       6   5.527966\n",
       "15       6   6.363936\n",
       "16       7   6.796256\n",
       "17       5   5.828408\n",
       "18       5   5.928893\n",
       "19       4   5.092832\n",
       "20       7   6.294611\n",
       "21       6   5.207945\n",
       "22       6   5.637689\n",
       "23       4   6.106051\n",
       "24       6   5.531360"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: Linear Regression on predicting wine quality revisited (cont)\n",
    "\n",
    "### YOUR CODE HERE. 1 line of code. 'y_pred' is the predicted value on test set.\n",
    "y_pred = None\n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Compare actual and predicted values\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df1 = df.head(25)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## c. KFold\n",
    "\n",
    "------------------\n",
    "\n",
    "### KFold(n_splits=3, shuffle=False)\n",
    "*  K-Folds cross-validator object.\n",
    "\n",
    "    Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "    Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    n_splits: number of folds, must be at least 2.\n",
    " 2.    shuffle: whether to shuffle the data before splitting. \n",
    " \n",
    "*  #### Methods: (of the KFold object)\n",
    "        * split(X, y=None): generate indices to split data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "# Example: cut the dataset into 1 half for train and 1 half for test\n",
    "\n",
    "# Data: \n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of Lasso regression with alpha value 2.2410206769492604 is:  115817.5853601778\n",
      "Error of Lasso regression with alpha value 2.2410206769492604 is:  112055.53774402301\n",
      "Error of Lasso regression with alpha value 2.2410206769492604 is:  178887.24366243515\n",
      "Error of Lasso regression with alpha value 2.2410206769492604 is:  74225.05711476142\n",
      "Error of Lasso regression with alpha value 2.2410206769492604 is:  112757.52432805097\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Linear Regression on Hitters dataset revisited\n",
    "\n",
    "# Reload everything\n",
    "# Read the data and remove rows with 'NaN' values.\n",
    "df = pd.read_csv('Hitters.csv').dropna()\n",
    "\n",
    "y = df.Salary.values\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "X_.dropna()\n",
    "\n",
    "X = X_[X_.columns].values\n",
    "\n",
    "\n",
    "# Good L1-regularization weight, we will see how this value is chosen shortly\n",
    "best_alpha = 2.2410206769492604\n",
    "\n",
    "# Define a L1-regularozation LinearRegression object with maximum 10000 iterations and normalization.\n",
    "# The regularization weight not specified yet.\n",
    "### YOUR CODE HERE. 1 line of code. \n",
    "lasso = None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# Define Kfold object, in which we wish to split the data into 5 subsets. We also want to shuffle it before splitting.\n",
    "### YOUR CODE HERE. 1 line of code.\n",
    "kf = None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "# Do L1-regularization Linear Regression 5 times\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "for train_index, test_index in None:\n",
    "    \n",
    "    # 2 lines of code\n",
    "    X_train, X_test = None\n",
    "    y_train, y_test = None\n",
    "    \n",
    "    # Set the regularization weight to be the value defined above\n",
    "    None\n",
    "    \n",
    "    # Hint: fit our model to the data.\n",
    "    None\n",
    "    \n",
    "    # Hint: 'y_pred' should be the predicted values on the test set.\n",
    "    y_pred = None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "    # Compute Mean-squarred-error:\n",
    "    print(\"Error of Lasso regression with alpha value \" + str(best_alpha) + \" is:  \" + str(mean_squared_error(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## d. LeaveOneOut\n",
    "\n",
    "------------------\n",
    "\n",
    "### LeaveOneOut()\n",
    "*  Leave-one-out cross-validator object.\n",
    "\n",
    "    Provides train/test indices to split data in train/test sets. Each sample is used once as a test set (singleton) while the remaining samples form the training set.\n",
    " \n",
    "*  #### Methods: (of the KFold object)\n",
    "        * split(X, y=None): generate indices to split data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn:  1\n",
      "Train set: \n",
      " [[ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] [2 3 4 5 6]\n",
      "Test set: \n",
      " [[1 2]] [1]\n",
      "\n",
      "\n",
      "Turn:  2\n",
      "Train set: \n",
      " [[ 1  2]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] [1 3 4 5 6]\n",
      "Test set: \n",
      " [[3 4]] [2]\n",
      "\n",
      "\n",
      "Turn:  3\n",
      "Train set: \n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] [1 2 4 5 6]\n",
      "Test set: \n",
      " [[5 6]] [3]\n",
      "\n",
      "\n",
      "Turn:  4\n",
      "Train set: \n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 9 10]\n",
      " [11 12]] [1 2 3 5 6]\n",
      "Test set: \n",
      " [[7 8]] [4]\n",
      "\n",
      "\n",
      "Turn:  5\n",
      "Train set: \n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [11 12]] [1 2 3 4 6]\n",
      "Test set: \n",
      " [[ 9 10]] [5]\n",
      "\n",
      "\n",
      "Turn:  6\n",
      "Train set: \n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]] [1 2 3 4 5]\n",
      "Test set: \n",
      " [[11 12]] [6]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: \n",
    "X = np.array([[1, 2], [3, 4], [5,6], [7,8], [9,10], [11,12]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "i = 0 \n",
    "for train_index, test_index in loo.split(X):\n",
    "    i += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"Turn: \", i)\n",
    "    print(\"Train set: \\n\", X_train, y_train)\n",
    "    print(\"Test set: \\n\", X_test, y_test)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Leave-One-Out Logistic Regression\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Drop the columns that are not numerical data\n",
    "df = df.drop([\"date\", \"time\", \"username\"], axis=1)\n",
    "\n",
    "data = df.values\n",
    "X = data[19000:19041, 1:]  # all rows, no label\n",
    "y = data[19000:19041, 0]  # all rows, label only\n",
    "scores = []\n",
    "i = 0 \n",
    "for train_index, test_index in loo.split(X):\n",
    "    i += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: can we use KFold() to apply Leave-One-Out?\n",
    "\n",
    "Answer: yes, just set n_splits equal to the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## e. Grid Search Cross Validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### GridSearchCV(estimator, param_grid, scoring=None, cv=’warn’, return_train_score=False)\n",
    "* Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "    Important members are fit, predict.\n",
    "\n",
    "    GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "    The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    estimator: estimator object; either estimator needs to provide a score function, or scoring must be passed.\n",
    " 2.    param_grid: dict of list of dictionaries\n",
    "       dictionary with parameters names (string) as keys and lists of parameter settings to try as vallues.\n",
    " 3.    scoring: a single string or a callable to evaluate the predictions.\n",
    "     * string: define model evaluation metric\n",
    "           * Classification:\n",
    "               * 'accuracy': accuracy classification score\n",
    "               * 'f1': F1-score\n",
    "               * 'precision': precision ratio true_positive/(true_positive + false_positive)\n",
    "               * 'recall': recall ratio true_positive/(true_positive + false_negative)\n",
    "           * Regression:\n",
    "               * 'explained_variance': explained variance regression score function.\n",
    "               * 'neg_mean_absolute_error': mean absolute error regression loss.\n",
    "               * 'neg_mean_squared_error': mean squared error regression loss.\n",
    "               \n",
    "     * callable: a function to evaluate the predictions on the test set\n",
    " 4.     cv: determines the cross-validating splitting strategy. \n",
    "         * None: default 3-fold cross validation.\n",
    "         * integer: specify the number of folds in a KFold\n",
    "         * an iterable yielding (train,test) splits as arrays of indices.\n",
    " 5.     return_train_score: whether to include training score.\n",
    "       \n",
    " \n",
    "*  #### Attributes (of the GridSearchCV object):\n",
    " *    cv_results_: a dict with keys as column heaers and values as columns, that can be imported into a pandas DataFrame.\n",
    " *    best_estimator_: estimator that was chosen by the search (associated with highest score/smallest loss)\n",
    " *    best_score_: mean cross-validated score of the best_estimator\n",
    " *    best_params_: parameter setting that gave the best results on the hold out data.\n",
    " *    best_index_: the index of the cv_results) array which corresponds to the best candidate parameter setting. \n",
    " *    n_splits: the number of cross-validation splits.\n",
    " \n",
    "*  #### Methods (on the GridSearchCV object)\n",
    " *    decision_function(X): call decision function on the estimator with the best found parameters.\n",
    " *    fit(X,y): run fit with all sets of parameters.\n",
    " *    get_params: get parameters for this estimator.\n",
    " *    inverse_transform(X): call inverse transform on the estimator with the best found parameters.\n",
    " *    predict(X): call predict on the estimator with the best found parameters.\n",
    " *    score(X, y): return the score on the given data, if the estimator as been refit.\n",
    " *    set_params(): set the parameters on this estimator.\n",
    " *    transform(X): call transform on the estimator with the best found parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                             max_iter=1000, normalize=False, positive=False,\n",
       "                             precompute=False, random_state=None,\n",
       "                             selection='cyclic', tol=0.0001, warm_start=False),\n",
       "             iid=False, n_jobs=None,\n",
       "             param_grid=[{'alpha': [0.5, 1, 1.5, 2, 2.5, 3],\n",
       "                          'max_iter': [100000, 200000], 'normalize': [True]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Grid Search Validation for Linear Regression\n",
    "\n",
    "# Read the data and remove rows with 'NaN' values.\n",
    "df = pd.read_csv('Hitters.csv').dropna()\n",
    "\n",
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Split the train and test set with ratio 50%, 50%, respectively.\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "parameter_candidates = [{'alpha': [0.5, 1, 1.5, 2, 2.5, 3], 'max_iter': [100000, 200000], 'normalize': [True]}]\n",
    "\n",
    "# Create a regression object on training data\n",
    "clf = GridSearchCV(estimator=Lasso(), param_grid=parameter_candidates, scoring = \"neg_mean_absolute_error\", cv=3, iid=False)\n",
    "\n",
    "# Train the classifier on data1's feature and target data\n",
    "### YOUR CODE HERE. 1 line of code.\n",
    "None\n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1\n"
     ]
    }
   ],
   "source": [
    "# Example: Grid Search Validation for Linear Regression (cont)\n",
    "\n",
    "print('Best alpha:',clf.best_estimator_.alpha) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Grid Search Validation for Linear Regression (cont)\n",
    "\n",
    "# Do GridSearch Cross validation with Ridge Linear Regression\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "# Define parameter_candidates for the parameter param_grid\n",
    "# with option to choose maximum number of iterations between 100000 and 200000\n",
    "# with option to choose alpha among [0, 0.5, 1, 1.5, 2, 2.5]\n",
    "# and normalization must be done\n",
    "parameter_candidates = [{None}]\n",
    "\n",
    "# Hint: Create a classifier object with the classifier and parameter candidates, mean-squarred error should be used\n",
    "clf = None\n",
    "\n",
    "# Hint: Train the classifier on training data\n",
    "None\n",
    "\n",
    "# Hint: Print the best alpha:\n",
    "print('Best alpha:', None) \n",
    "\n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## f. Randomized Search Cross Validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### RandomizedSearchCV(estimator, param_distributions,n_iter=10, scoring=None, cv=’warn’, return_train_score=False)\n",
    "* Randomized search on hyper parameters.\n",
    "\n",
    "    The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n",
    "\n",
    "    In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "    If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    estimator: estimator object; either estimator needs to provide a score function, or scoring must be passed.\n",
    " 2.    param_distributions: dictionary with parameters names (string) as keys and distributions/lists of parameters to try.\n",
    " 3.    n_iter: number of parameter settings that are sampled.\n",
    " 4.    scoring: a single string or a callable to evaluate the predictions.\n",
    "     * string: define model evaluation metric\n",
    "           * Classification:\n",
    "               * 'accuracy': accuracy classification score\n",
    "               * 'f1': F1-score\n",
    "               * 'precision': precision ratio true_positive/(true_positive + false_positive)\n",
    "               * 'recall': recall ratio true_positive/(true_positive + false_negative)\n",
    "           * Regression:\n",
    "               * 'explained_variance': explained variance regression score function.\n",
    "               * 'neg_mean_absolute_error': mean absolute error regression loss.\n",
    "               * 'neg_mean_squared_error': mean squared error regression loss.\n",
    "               \n",
    "     * callable: a function to evaluate the predictions on the test set\n",
    " 5.     cv: determines the cross-validating splitting strategy. \n",
    "         * None: default 3-fold cross validation.\n",
    "         * integer: specify the number of folds in a KFold\n",
    "         * an iterable yielding (train,test) splits as arrays of indices.\n",
    " 6.     return_train_score: whether to include training score.\n",
    "       \n",
    " \n",
    "*  #### Attributes (of the RandomizedSearchCV object):\n",
    " *    cv_results_: a dict with keys as column heaers and values as columns, that can be imported into a pandas DataFrame.\n",
    " *    best_estimator_: estimator that was chosen by the search (associated with highest score/smallest loss)\n",
    " *    best_score_: mean cross-validated score of the best_estimator\n",
    " *    best_params_: parameter setting that gave the best results on the hold out data.\n",
    " *    best_index_: the index of the cv_results) array which corresponds to the best candidate parameter setting. \n",
    " *    n_splits: the number of cross-validation splits.\n",
    " \n",
    "*  #### Methods (on the RandomizedSearchCV object)\n",
    " *    decision_function(X): call decision function on the estimator with the best found parameters.\n",
    " *    fit(X,y): run fit with all sets of parameters.\n",
    " *    get_params: get parameters for this estimator.\n",
    " *    inverse_transform(X): call inverse transform on the estimator with the best found parameters.\n",
    " *    predict(X): call predict on the estimator with the best found parameters.\n",
    " *    score(X, y): return the score on the given data, if the estimator as been refit.\n",
    " *    set_params(): set the parameters on this estimator.\n",
    " *    transform(X): call transform on the estimator with the best found parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Randomized Search Cross Validation with Lasso Linear Regression\n",
    "\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "# Define a dictionary so that maximum number of iterations is a list of 10000 and 100000\n",
    "# and alpha is a range of 0.5, 0.6, ..., 5.4, 5.5 (Hint: use np.arange())\n",
    "# and normalization must be done\n",
    "parameter_candidates = None\n",
    "\n",
    "# Hint: Create a classifier object with the classifier and parameter candidates, 50 parameter settings are examined.\n",
    "clf = None\n",
    "\n",
    "# Train the classifier on training data\n",
    "None\n",
    "\n",
    "# Print the best alpha:\n",
    "print('Best alpha:', None) \n",
    "\n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## g. Lasso Regression cross-validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### LassoCV(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=’warn’)\n",
    "* Lasso linear model with iterative fitting along a regularization path.\n",
    "    \n",
    "    The best model is selected by cross-validation.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    eps: float, length of the path, = alpha_min/alpha_max.\n",
    " 2.    param_grid: dict of list of dictionaries\n",
    " 3.    fit_intercept: whether to calculate the intercept. If False, data is supposed to be centered.\n",
    " 4.    normalize: whether to normalize; ignored when fit_intercept=False\n",
    " 5.    max_iter : maximum number of iterations.\n",
    " 6.    tol: tolerance for the optimization\n",
    " 7.    cv: determines the cross-validating splitting strategy. \n",
    "         * None: default 3-fold cross validation.\n",
    "         * integer: specify the number of folds in a KFold\n",
    "         * an iterable yielding (train,test) splits as arrays of indices.\n",
    "       \n",
    " \n",
    "*  #### Attributes (of the LassoCV object):\n",
    " *    alpha_: weight of regularization\n",
    " *    coef_: trained parameter vector\n",
    " *    intercept_: trained intercept\n",
    " *    alphas_: the grid of alphas used for fitting.\n",
    " \n",
    "*  #### Methods (on the LassoCV object)\n",
    " *    fit(X,y): run fit with all sets of parameters.\n",
    " *    get_params: get parameters for this estimator.\n",
    " *    predict(X): call predict on the estimator with the best found parameters.\n",
    " *    score(X, y): return the score on the given data, if the estimator as been refit.\n",
    " *    set_params(): set the parameters on this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2410206769492604\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Lasso Linear Regression revisited\n",
    "\n",
    "# In Exercise 1 of Lasso Linear Regression, a value 'best_alpha' =  2.2410206769492604 without any explanations.\n",
    "\n",
    "# Let us find it by Lasso Cross-validation\n",
    "\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "# Hint: define a Lasso cross-validation object, 10 folds, maximum 100000 iterations, normalization and no specified range of alpha, \n",
    "lassocv = None\n",
    "# Hint: train it on the train set\n",
    "None\n",
    "# Hint: print out the optimal alpha\n",
    "print(None)\n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the value that we used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## h. Ridge Regression cross-validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None)\n",
    "* Ridge regression with built-in cross-validation.\n",
    "\n",
    "\n",
    "    By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    alphas: array pf alpha values to try, weight of regularization.\n",
    " 2.    fit_intercept: whether to calculate the intercept. If False, data is supposed to be centered.\n",
    " 3.    normalize: whether to normalize; ignored when fit_intercept=False\n",
    " 4.    scoring: a single string or a callable to evaluate the predictions.\n",
    "     * string: define model evaluation metric\n",
    "           * Classification:\n",
    "               * 'accuracy': accuracy classification score\n",
    "               * 'f1': F1-score\n",
    "               * 'precision': precision ratio true_positive/(true_positive + false_positive)\n",
    "               * 'recall': recall ratio true_positive/(true_positive + false_negative)\n",
    "           * Regression:\n",
    "               * 'explained_variance': explained variance regression score function.\n",
    "               * 'neg_mean_absolute_error': mean absolute error regression loss.\n",
    "               * 'neg_mean_squared_error': mean squared error regression loss.\n",
    " 5.    cv: determines the cross-validating splitting strategy. \n",
    "         * None: default 3-fold cross validation.\n",
    "         * integer: specify the number of folds in a KFold\n",
    "         * an iterable yielding (train,test) splits as arrays of indices.\n",
    "       \n",
    " \n",
    "*  #### Attributes (of the RidgeCV object):\n",
    " *    alpha_: weight of regularization\n",
    " *    coef_: trained parameter vector\n",
    " *    intercept_: trained intercept\n",
    " *    cv_values_: cross-validation values for each alpha. after fit() called, contain the mean squared errors.\n",
    "\n",
    "*  #### Methods (on the RidgeCV object)\n",
    " *    fit(X,y): run fit with all sets of parameters.\n",
    " *    get_params: get parameters for this estimator.\n",
    " *    predict(X): call predict on the estimator with the best found parameters.\n",
    " *    score(X, y): return the score on the given data, if the estimator as been refit.\n",
    " *    set_params(): set the parameters on this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5748784976988678\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Ridge Linear Regression revisited\n",
    "\n",
    "# Similarly, in Exercise 3 of Ridge Linear Regression, a value 'best_alpha' =  0.5748784976988678 without any explanations.\n",
    "\n",
    "# Let us find it by Ridge Cross-validation\n",
    "\n",
    "# Range of alpha values to consider\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "# Hint: define a Lasso cross-validation object, mean squared error metric, with normalization and range of alpha values defined above\n",
    "ridgecv = None\n",
    "# Hint: train it on the train set\n",
    "None\n",
    "# Hint: print out the optimal alpha\n",
    "print(None)\n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also the value that we used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## i. Logistic Regression cross-validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### LogisticRegressionCV(Cs=10, fit_intercept=True, cv=’warn’, penalty=’l2’, scoring=None, solver=’lbfgs’, tol=0.0001, max_iter=100, 1_ratios=None)\n",
    "* Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    Cs: each of the values in Cs describes the inverse of regularization strength.\n",
    " 2.    fit_intercept: whether to calculate the intercept. If False, data is supposed to be centered.\n",
    " 3.    cv: determines the cross-validating splitting strategy. \n",
    "         * None: default 3-fold cross validation.\n",
    "         * integer: specify the number of folds in a KFold\n",
    "         * an iterable yielding (train,test) splits as arrays of indices.\n",
    " 4.    penalty: type of regularization (\"l1\", \"l2\", \"elasticnet\")\n",
    " 5.    scoring: a single string or a callable to evaluate the predictions.\n",
    "     * string: define model evaluation metric\n",
    "           * Classification:\n",
    "               * 'accuracy': accuracy classification score\n",
    "               * 'f1': F1-score\n",
    "               * 'precision': precision ratio true_positive/(true_positive + false_positive)\n",
    "               * 'recall': recall ratio true_positive/(true_positive + false_negative)\n",
    "           * Regression:\n",
    "               * 'explained_variance': explained variance regression score function.\n",
    "               * 'neg_mean_absolute_error': mean absolute error regression loss.\n",
    "               * 'neg_mean_squared_error': mean squared error regression loss.\n",
    " 6.    solver: optimization algorithm (‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’)\n",
    " 7.    tol: tolerance for stopping criteria\n",
    " 8.    max_iter: maximum number of iterations.\n",
    " 9.    l1_ratios: list of floats, ratio of l1-regularization, only applicable when penalty='elasticnet'.\n",
    " \n",
    "*  #### Attributes (of the LogisticRegressionCV object):\n",
    " *    classes_: a list of class labels known to the classifier.\n",
    " *    coef_: trained parameter vector\n",
    " *    intercept_: trained intercept\n",
    " *    Cs_: array of C i.e. inverse of regularization parameter values used for cross-validation.\n",
    " *    l1_ratios_: array of l1_ratios used for cross-validation.\n",
    " *    scores_: dict with classes as keys, values as grid of scores obtained during cross-validating each fold.\n",
    " *    C_: array of C that maps to the best scores across every class.\n",
    " *    l1_ratio_: array of l1_ratio_ that maps to the best scores across every class.\n",
    "\n",
    "*  #### Methods (on the LogisticRegressionCV object)\n",
    " *    decision_function(X): predict confidence scores for samples.\n",
    " *    fit(X,y): fit the model according to the given training data.\n",
    " *    get_params: get parameters for this estimator.\n",
    " *    predict(X): predict class labels for samples in X.\n",
    " *    score(X, y): return the score using the scoring option on the given test data and labels.\n",
    " *    set_params(): set the parameters on this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 26 columns):\n",
      "Unnamed: 0    30000 non-null int64\n",
      "ID            30000 non-null int64\n",
      "LIMIT_BAL     30000 non-null float64\n",
      "SEX           30000 non-null float64\n",
      "EDUCATION     30000 non-null float64\n",
      "MARRIAGE      30000 non-null float64\n",
      "AGE           30000 non-null float64\n",
      "PAY_0         30000 non-null float64\n",
      "PAY_2         30000 non-null float64\n",
      "PAY_3         30000 non-null float64\n",
      "PAY_4         30000 non-null float64\n",
      "PAY_5         30000 non-null float64\n",
      "PAY_6         30000 non-null float64\n",
      "BILL_AMT1     30000 non-null float64\n",
      "BILL_AMT2     30000 non-null float64\n",
      "BILL_AMT3     30000 non-null float64\n",
      "BILL_AMT4     30000 non-null float64\n",
      "BILL_AMT5     30000 non-null float64\n",
      "BILL_AMT6     30000 non-null float64\n",
      "PAY_AMT1      30000 non-null float64\n",
      "PAY_AMT2      30000 non-null float64\n",
      "PAY_AMT3      30000 non-null float64\n",
      "PAY_AMT4      30000 non-null float64\n",
      "PAY_AMT5      30000 non-null float64\n",
      "PAY_AMT6      30000 non-null float64\n",
      "target        30000 non-null int64\n",
      "dtypes: float64(23), int64(3)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Exercise: simple LogisticRegressionCV\n",
    "\n",
    "# Load the dataset\n",
    "df1=pd.read_csv('dataset_train_woed.csv')\n",
    "\n",
    "X=df1.drop(['Unnamed: 0','ID','target'],axis=1).values\n",
    "y=df1.target.values\n",
    "\n",
    "# Some insights into the data\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175\n",
      "0.8186666666666667\n",
      "0.8146666666666667\n",
      "0.8231666666666667\n",
      "0.8178333333333333\n"
     ]
    }
   ],
   "source": [
    "# Exercise: simple LogisticRegressionCV\n",
    "\n",
    "# Define a K-Fold object of 5 folds, do shuffling before spliting\n",
    "### YOUR CODE HERE. 1 line of code\n",
    "kf = None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "for train_index, test_index in None:\n",
    "    X_train, X_test = None\n",
    "    y_train, y_test = None\n",
    "\n",
    "    # 3 lines of code. Do LogisticRegressionCV. \n",
    "    clf = None\n",
    "    None\n",
    "    y_pred = None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(acc)\n",
    "    \n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "## j. Ridge classifier cross-validation\n",
    "\n",
    "------------------\n",
    "\n",
    "### RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None)\n",
    "* Ridge classifier with built-in cross-validation.\n",
    " \n",
    "* #### Parameters\n",
    " 1.    alphas: array of alpha values, weights of regularization, to try.\n",
    " 2.    fit_intercept: whether to calculate the intercept. If False, data is supposed to be centered.\n",
    " 3.    normalize: whether to compute normalization; ignored when fit_intercept=False.\n",
    " 4.    scoring: a single string or a callable to evaluate the predictions.\n",
    "     * string: define model evaluation metric\n",
    "           * Classification:\n",
    "               * 'accuracy': accuracy classification score\n",
    "               * 'f1': F1-score\n",
    "               * 'precision': precision ratio true_positive/(true_positive + false_positive)\n",
    "               * 'recall': recall ratio true_positive/(true_positive + false_negative)\n",
    "           * Regression:\n",
    "               * 'explained_variance': explained variance regression score function.\n",
    "               * 'neg_mean_absolute_error': mean absolute error regression loss.\n",
    "               * 'neg_mean_squared_error': mean squared error regression loss.\n",
    " 5.    cv: determines the cross-validating splitting strategy. \n",
    "         * None: default 3-fold cross validation.\n",
    "         * integer: specify the number of folds in a KFold\n",
    "         * an iterable yielding (train,test) splits as arrays of indices.\n",
    " \n",
    "*  #### Attributes (of the RidgeClassifierCV object):\n",
    " *    cv_values_: cross-validation values for each alpha. After fit() called, this contains the mean squared errors or the           value of loss function.\n",
    " *    coef_: trained parameter vector\n",
    " *    intercept_: trained intercept\n",
    " *    alpha_: estimated regularization parameter\n",
    "\n",
    "*  #### Methods (on the RidgeClassifierCV object)\n",
    " *    decision_function(X): predict confidence scores for samples.\n",
    " *    fit(X,y): fit the model according to the given training data.\n",
    " *    get_params: get parameters for this estimator.\n",
    " *    predict(X): predict class labels for samples in X.\n",
    " *    score(X, y): return the score using the scoring option on the given test data and labels.\n",
    " *    set_params(): set the parameters on this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n",
      "0.9788732394366197\n",
      "0.9436619718309859\n",
      "0.9577464788732394\n"
     ]
    }
   ],
   "source": [
    "# Exercise: simple RidgeClassifierCV\n",
    "\n",
    "# Load the data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Define a K-Fold object of 4 folds, do shuffling before spliting\n",
    "### YOUR CODE HERE. 1 line of code\n",
    "kf = None\n",
    "### END OF YOUR CODE.\n",
    "\n",
    "### YOUR CODE HERE. Fill in the \"None\".\n",
    "for train_index, test_index in None:\n",
    "    X_train, X_test = None\n",
    "    y_train, y_test = None\n",
    "\n",
    "    # 3 lines of code. Define a LogisticRegressionCV object and train it on the training data.\n",
    "    # Hint: make sure that alpha can have values: 0.001, 0.01, 0.1, 1\n",
    "    clf = None\n",
    "    None\n",
    "    # 'score' is the average accuracy\n",
    "    score = None\n",
    "    print(score)\n",
    "    \n",
    "### END OF YOUR CODE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "2. https://scikit-learn.org/stable/modules/model_evaluation.html#scoring\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection\n",
    "4. https://chrisalbon.com/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search/\n",
    "5. https://www.kaggle.com/wilsonf/uci-credit-carefrom-python-woe-pkg/downloads/UCI_Credit_Card.csv/1\n",
    "6. https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the end of today class. We hope you had fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
